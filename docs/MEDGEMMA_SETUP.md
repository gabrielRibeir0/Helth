# Guia de Implementa√ß√£o MedGemma

## üéØ Vis√£o Geral

O **MedGemma** √© uma fam√≠lia de modelos de linguagem especializados em medicina, desenvolvidos pela Google DeepMind. Est√£o otimizados para tarefas m√©dicas e cl√≠nicas.

Este guia cobre a implementa√ß√£o no sistema HELTH com **3 op√ß√µes de deployment**.

---

## üìã Pr√©-requisitos

### Op√ß√£o 1: Ollama (Recomendado para In√≠cio)
- ‚úÖ **Mais Simples**: Setup em 2 minutos
- ‚úÖ **Gratuito**: Deployment local
- ‚úÖ **Bom para**: Desenvolvimento e prototipagem

**Requisitos:**
```bash
# 1. Instalar Ollama
# macOS/Linux: https://ollama.ai
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Baixar MedGemma
ollama pull medgemma

# 3. Verificar (servidor roda automaticamente em localhost:11434)
ollama list
```

### Op√ß√£o 2: HuggingFace (Controlo Total)
- ‚úÖ **Flex√≠vel**: Deployment local ou cloud
- ‚úÖ **Gratuito**: Modelos open-source
- ‚ö†Ô∏è **Requer**: GPU com ‚â•8GB VRAM (2B) ou ‚â•16GB (7B)

**Requisitos:**
```bash
# Instalar depend√™ncias
pip install transformers accelerate bitsandbytes torch sentencepiece

# GPU NVIDIA necess√°ria (verificar)
nvidia-smi
```

### Op√ß√£o 3: Google Vertex AI (Produ√ß√£o Cloud)
- ‚úÖ **Escal√°vel**: Managed service
- ‚úÖ **Sem infraestrutura**: Google gere tudo
- ‚ö†Ô∏è **Pago**: Cobrado por uso

**Requisitos:**
```bash
# 1. Conta GCP ativa
# 2. Vertex AI API ativada
gcloud services enable aiplatform.googleapis.com

# 3. Autentica√ß√£o
gcloud auth application-default login

# 4. Instalar SDK
pip install langchain-google-vertexai google-cloud-aiplatform
```

---

## üöÄ Instala√ß√£o

### 1. Clonar/Atualizar Reposit√≥rio

```bash
cd sns24
git pull  # se j√° tiver o reposit√≥rio
```

### 2. Instalar Depend√™ncias

Escolha UMA das seguintes op√ß√µes:

#### Op√ß√£o A: Ollama (Simples)
```bash
# Depend√™ncias base
pip install -e .

# MedGemma via Ollama (j√° instalado nos pr√©-requisitos)
ollama pull medgemma
```

#### Op√ß√£o B: HuggingFace (Local)
```bash
# Todas as depend√™ncias (inclui PyTorch, Transformers, etc.)
pip install -e .

# OU instalar apenas extras para HuggingFace
pip install transformers accelerate bitsandbytes torch sentencepiece
```

#### Op√ß√£o C: Vertex AI (Cloud)
```bash
# Depend√™ncias base
pip install -e .

# Extras para Vertex AI
pip install langchain-google-vertexai google-cloud-aiplatform
```

### 3. Configurar Vari√°veis de Ambiente

```bash
# Copiar template
cp .env.example .env

# Editar .env
nano .env
```

Adicionar ao `.env`:
```bash
# LLM Provider (escolher um)
MEDGEMMA_PROVIDER=ollama  # ou "huggingface" ou "vertexai"
MEDGEMMA_MODEL_SIZE=2b    # "2b" ou "7b" (apenas HuggingFace)

# Google Cloud (apenas se MEDGEMMA_PROVIDER=vertexai)
GOOGLE_CLOUD_PROJECT=seu-projeto-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# Outras configura√ß√µes
POSTGRES_USER=helth_user
POSTGRES_PASSWORD=sua_senha
POSTGRES_DB=helth_db
```

---

## üíª Uso B√°sico

### Teste R√°pido (Ollama)

```python
from src.llm.medgemma import get_medgemma_llm

# Criar LLM
llm = get_medgemma_llm(provider="ollama")

# Testar
resposta = llm.invoke("O que √© hipertens√£o arterial?")
print(resposta)
```

### Teste R√°pido (HuggingFace)

```python
from src.llm.medgemma import get_medgemma_llm

# Criar LLM (primeira vez demora ~2-5 min para download)
llm = get_medgemma_llm(
    provider="huggingface",
    model_size="2b",  # ou "7b" se tiver GPU potente
    use_quantization=True  # Reduz uso de mem√≥ria
)

# Testar
resposta = llm.invoke("Quais os sintomas de diabetes tipo 2?")
print(resposta)
```

### Agente Completo

```python
from src.agents.nesy_agent import create_medgemma_agent
from langchain.tools import Tool

# Definir ferramentas
tools = [
    Tool(
        name="ConsultarVitais",
        func=lambda paciente_id: f"PA: 120/80, FC: 72, SpO2: 98%",
        description="Consulta sinais vitais do paciente"
    ),
]

# Criar agente
agent = create_medgemma_agent(
    tools=tools,
    db_postgres=None,  # suas conex√µes
    vector_db=None,
    mongo_db=None,
    provider="ollama"  # ou "huggingface", "vertexai"
)

# Usar
resposta = agent.query("Consulta vitais do paciente 001 e avalia")
print(resposta)
```

---

## üéõÔ∏è Configura√ß√µes Avan√ßadas

### Controlo de Temperatura

```python
# Mais conservador (produ√ß√£o m√©dica)
llm = get_medgemma_llm(
    provider="huggingface",
    model_size="7b",
    temperature=0.3  # Respostas mais determin√≠sticas
)

# Mais criativo (explora√ß√£o)
llm = get_medgemma_llm(
    provider="huggingface",
    model_size="2b",
    temperature=0.9  # Respostas mais variadas
)
```

### Otimiza√ß√£o de Mem√≥ria (HuggingFace)

```python
from src.llm.medgemma import MedGemmaHuggingFace

# Quantiza√ß√£o 4-bit (recomendado para GPUs <16GB)
llm = MedGemmaHuggingFace(
    model_name="google/medgemma-7b",
    use_quantization=True,  # Reduz de ~14GB para ~4GB
    device="auto"
)

# Sem quantiza√ß√£o (apenas se tiver VRAM suficiente)
llm = MedGemmaHuggingFace(
    model_name="google/medgemma-7b",
    use_quantization=False,
    device="cuda"
)
```

### CPU-only (Lento mas Poss√≠vel)

```python
llm = get_medgemma_llm(
    provider="huggingface",
    model_size="2b",
    device="cpu",
    use_quantization=True
)
# ‚ö†Ô∏è Aviso: 10-50x mais lento que GPU
```

---

## üìä Compara√ß√£o de Op√ß√µes

| Crit√©rio | Ollama | HuggingFace | Vertex AI |
|----------|--------|-------------|-----------|
| **Setup** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Simples | ‚≠ê‚≠ê‚≠ê M√©dio | ‚≠ê‚≠ê Complexo |
| **Custo** | üí∞ Gratuito | üí∞ Gratuito | üí∞üí∞üí∞ Pago |
| **Velocidade** | ‚ö°‚ö°‚ö° R√°pida | ‚ö°‚ö°‚ö°‚ö° Muito r√°pida | ‚ö°‚ö°‚ö°‚ö°‚ö° Escal√°vel |
| **Controlo** | ‚≠ê‚≠ê B√°sico | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Total | ‚≠ê‚≠ê‚≠ê M√©dio |
| **GPU Necess√°ria** | ‚ùå N√£o | ‚úÖ Sim (8-16GB) | ‚ùå N√£o (cloud) |
| **Internet** | ‚ùå N√£o | ‚ùå N√£o* | ‚úÖ Sim |
| **Melhor para** | Dev/Prototipagem | Produ√ß√£o local | Produ√ß√£o cloud |

*Ap√≥s download inicial

---

## üîß Troubleshooting

### Problema: "CUDA out of memory"
```python
# Solu√ß√£o: Usar quantiza√ß√£o ou modelo menor
llm = get_medgemma_llm(
    provider="huggingface",
    model_size="2b",  # trocar de 7b para 2b
    use_quantization=True
)
```

### Problema: Ollama n√£o conecta
```bash
# Verificar se servidor est√° a correr
curl http://localhost:11434/api/tags

# Reiniciar Ollama
ollama serve

# Verificar se modelo foi baixado
ollama list
```

### Problema: Download lento (HuggingFace)
```python
# Definir mirror (se fora dos EUA)
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
```

---

## üìö Recursos Adicionais

- **Documenta√ß√£o MedGemma**: https://deepmind.google/models/gemma/medgemma/
- **HuggingFace Hub**: https://huggingface.co/google/medgemma-2b
- **Ollama**: https://ollama.ai
- **Exemplos de C√≥digo**: `/examples/medgemma_usage.py`

---

## üéì Pr√≥ximos Passos

1. ‚úÖ **Testar localmente**: Use Ollama para validar funcionamento
2. ‚úÖ **Adicionar ferramentas**: Criar queries SQL, RAG, calculadoras
3. ‚úÖ **Integrar interface**: Conectar ao Streamlit ([app.py](app.py))
4. ‚úÖ **Deploy produ√ß√£o**: Escolher HuggingFace (local) ou Vertex AI (cloud)

---

**D√∫vidas?** Ver exemplos completos em [`examples/medgemma_usage.py`](../examples/medgemma_usage.py)
